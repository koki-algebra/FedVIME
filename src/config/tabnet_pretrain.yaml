# The unique identifier for each federated learning task
task_id: ""

test_mode: "test_in_client"
# The way to measure testing performance (accuracy) when test mode is `test_in_client`, support: average or weighted (means weighted average).
test_method: "average"

server:
  track: False  # Whether track server metrics using the tracking service.
  rounds: 10  # Total training round.
  clients_per_round: 5  # The number of clients to train in each round.
  test_every: 1  # The frequency of testing: conduct testing every N round.
  save_model_every: 10  # The frequency of saving model: save model every N round.
  save_model_path: ""  # The path to save model. Default path is root directory of the library.
  batch_size: 32  # The batch size of test_in_server.
  test_all: False  # Whether test all clients or only selected clients.
  random_selection: True  # Whether select clients to train randomly.
  # The strategy to aggregate client uploaded models, options: FedAvg, equal.
    # FedAvg aggregates models using weighted average, where the weights are data size of clients.
    # equal aggregates model by simple averaging.
  aggregation_stragtegy: "FedAvg"
  # The content of aggregation, options: all, parameters.
    # all means aggregating models using state_dict, including both model parameters and persistent buffers like BatchNorm stats.
    # parameters means aggregating only model parameters.
  aggregation_content: "all"

client:
  track: False  # Whether track server metrics using the tracking service.
  batch_size: 32  # The batch size of training in client.
  test_batch_size: 5  # The batch size of testing in client.
  local_epoch: 10  # The number of epochs to train in each round.
  optimizer:
    type: "Adam"  # The name of the optimizer, options: Adam, SGD.
    lr: 0.001
    momentum: 0.9
    weight_decay: 0
  seed: 0
  local_test: False  # Whether test the trained models in clients before uploading them to the server.

gpu: 0  # The total number of GPUs used in training. 0 means CPU.
distributed:  # The distributed training configurations. It is only applicable when gpu > 1.
  backend: "nccl"  # The distributed backend.
  init_method: ""
  world_size: 0
  rank: 0
  local_rank: 0

tracking:  # The configurations for logging and tracking.
  database: ""  # The path of local dataset, sqlite3.
  log_file: ""
  log_level: "INFO"  # The level of logging.
  metric_file: ""
  save_every: 1


# model parameters
model_parameters:
  pretraining_ratio: 0.5  # Between 0 and 1, percentage of feature to mask for reconstruction
  n_d: 8  # Width of the decision prediction layer. Bigger values gives more capacity to the model with the risk of overfitting. Values typically range from 8 to 64.
  n_a: 8  # Width of the attention embedding for each mask. According to the paper n_d=n_a is usually a good choice. (default=8)
  n_steps: 3  # Number of steps in the architecture (usually between 3 and 10)
  gamma: 1.3  # This is the coefficient for feature reusage in the masks. A value close to 1 will make mask selection least correlated between layers. Values range from 1.0 to 2.0.
  n_independent: 2  # Number of independent Gated Linear Units layers at each step. Usual values range from 1 to 5.
  n_shared: 2  # Number of shared Gated Linear Units at each step Usual values range from 1 to 5
  epsilon: 1e-15  # Should be left untouched.
  virtual_batch_size: 128  # Size of the mini batches used for "Ghost Batch Normalization".
  momentum: 0.02  # Momentum for batch normalization, typically ranges from 0.01 to 0.4 (default=0.02)
  mask_type: "sparsemax"  # Either "sparsemax" or "entmax" : this is the masking function to use for selecting features.
  n_shared_decoder: 1  # Number of shared GLU block in decoder.
  n_indep_decoder: 1  # Number of independent GLU block in decoder.
